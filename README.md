# MMAPEAK - CUDA Matrix Multiply Performance Benchmark


# MMAPEAK æ€§èƒ½å¯è§†åŒ–é¢æ¿

è¿™æ˜¯ä¸€ä¸ªåŸºäº Vue 3 + Vite + AntV çš„ CUDA çŸ©é˜µä¹˜æ³•æ€§èƒ½æµ‹è¯•ç»“æœå¯è§†åŒ–åº”ç”¨ç¨‹åºã€‚

![æ€§èƒ½å¯¹æ¯”å›¾è¡¨](./performance-chart.png)

## åŠŸèƒ½ç‰¹æ€§

- ğŸ“Š **äº¤äº’å¼å›¾è¡¨**: ä½¿ç”¨ AntV G2 åˆ›å»ºç¾è§‚çš„æŸ±çŠ¶å›¾
- ğŸ“‹ **æ•°æ®è¡¨æ ¼**: è¯¦ç»†çš„æ€§èƒ½æ•°æ®è¡¨æ ¼å±•ç¤º
- ğŸ”„ **å®æ—¶åˆ‡æ¢**: æ”¯æŒæ€§èƒ½(TFLOPS)å’Œæ‰§è¡Œæ—¶é—´(ç§’)ä¸¤ç§è§†å›¾
- ğŸ“ **æ–‡ä»¶ä¸Šä¼ **: æ”¯æŒä¸Šä¼ è‡ªå®šä¹‰çš„æµ‹è¯•ç»“æœæ–‡ä»¶
- ğŸ¨ **å“åº”å¼è®¾è®¡**: æ”¯æŒæ¡Œé¢å’Œç§»åŠ¨è®¾å¤‡
- ğŸŒ **ä¸­æ–‡ç•Œé¢**: å®Œå…¨ä¸­æ–‡åŒ–çš„ç”¨æˆ·ç•Œé¢

## æ”¯æŒçš„æ•°æ®æ ¼å¼

æ”¯æŒè§£æä»¥ä¸‹æ“ä½œç±»å‹çš„æ€§èƒ½æ•°æ®ï¼š
- **4ä½ç²¾åº¦**: INT4, MXF4, NVF4, FP4
- **6ä½ç²¾åº¦**: FP6, MXF6  
- **8ä½ç²¾åº¦**: FP8, MXF8, INT8
- **16ä½ç²¾åº¦**: FP16, BF16
- **32ä½ç²¾åº¦**: TF32

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Node.js 18+
- pnpm

### å®‰è£…ä¾èµ–

```bash
pnpm install
```

### å¯åŠ¨å¼€å‘æœåŠ¡å™¨

```bash
pnpm run dev
```

åº”ç”¨å°†åœ¨ `http://localhost:5173` å¯åŠ¨

### æ„å»ºç”Ÿäº§ç‰ˆæœ¬

```bash
pnpm run build
```

## ä½¿ç”¨æ–¹æ³•

1. **åŠ è½½æ•°æ®**:
   - ç‚¹å‡»"ä½¿ç”¨ç¤ºä¾‹æ•°æ®"æŒ‰é’®æŸ¥çœ‹æ¼”ç¤º
   - æˆ–ä¸Šä¼ è‡ªå·±çš„ `.txt` æ ¼å¼æµ‹è¯•ç»“æœæ–‡ä»¶

2. **åˆ‡æ¢è§†å›¾**:
   - é€‰æ‹©"æ€§èƒ½ (TFLOPS)"æŸ¥çœ‹è®¡ç®—æ€§èƒ½å¯¹æ¯”
   - é€‰æ‹©"æ‰§è¡Œæ—¶é—´ (ç§’)"æŸ¥çœ‹æ—¶é—´æ¶ˆè€—å¯¹æ¯”

3. **æ•°æ®åˆ†æ**:
   - æŸ¥çœ‹è®¾å¤‡ä¿¡æ¯äº†è§£æµ‹è¯•ç¡¬ä»¶
   - åˆ†ææŸ±çŠ¶å›¾å¯¹æ¯”ä¸åŒç²¾åº¦æ ¼å¼çš„æ€§èƒ½
   - æŸ¥çœ‹è¯¦ç»†è¡¨æ ¼äº†è§£å…·ä½“æ•°å€¼

## æ•°æ®æ–‡ä»¶æ ¼å¼

æ”¯æŒçš„è¾“å…¥æ–‡ä»¶æ ¼å¼ç¤ºä¾‹ï¼š

```
----------------------------------------
Device 0: NVIDIA GeForce RTX 5090 D
  Compute capability: 12.0
  Total global memory: 31.8 GiB
  Multiprocessor count: 170
Running benchmarks with target time: 3.0 seconds
mma_s4s4s32_8_8_32
run: 2987.2 ms 75.8 T(fl)ops
mma_mxf4mxf4f32_16_8_64
run: 2974.2 ms 859.6 T(fl)ops
...
```

## æŠ€æœ¯æ ˆ

- **Vue 3**: ç°ä»£å‰ç«¯æ¡†æ¶
- **Vite**: å¿«é€Ÿæ„å»ºå·¥å…·
- **TypeScript**: ç±»å‹å®‰å…¨
- **AntV G2**: ä¸“ä¸šæ•°æ®å¯è§†åŒ–
- **CSS3**: ç°ä»£æ ·å¼è®¾è®¡

## é¡¹ç›®ç»“æ„

```
src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ PerformanceChart.vue    # å›¾è¡¨ç»„ä»¶
â”‚   â””â”€â”€ PerformanceTable.vue    # è¡¨æ ¼ç»„ä»¶
â”œâ”€â”€ data/
â”‚   â””â”€â”€ resultParser.ts         # æ•°æ®è§£ææ¨¡å—
â”œâ”€â”€ App.vue                     # ä¸»åº”ç”¨ç»„ä»¶
â””â”€â”€ main.ts                     # åº”ç”¨å…¥å£
```

## æ€§èƒ½æŒ‡æ ‡è¯´æ˜

- **TFLOPS**: æ¯ç§’ä¸‡äº¿æ¬¡æµ®ç‚¹è¿ç®—ï¼Œæ•°å€¼è¶Šé«˜æ€§èƒ½è¶Šå¥½
- **æ‰§è¡Œæ—¶é—´**: æ“ä½œå®Œæˆæ‰€éœ€æ—¶é—´ï¼Œæ—¶é—´è¶ŠçŸ­æ•ˆç‡è¶Šé«˜
- **ä¸åŒç²¾åº¦æ ¼å¼**: å¯¹æ¯”å„ç§ä½å®½çš„è®¡ç®—ç²¾åº¦å’Œæ€§èƒ½å¹³è¡¡

## RTX 5090 vs RTX 5090 D æ€§èƒ½å·®å¼‚åˆ†æ

### 4ä½ç²¾åº¦æ“ä½œæ€§èƒ½å¯¹æ¯”

åœ¨4ä½ç²¾åº¦çŸ©é˜µä¹˜æ³•æ“ä½œä¸­ï¼Œä¸¤æ¬¾æ˜¾å¡è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ï¼š

| æ“ä½œç±»å‹ | RTX 5090 D | RTX 5090 | æ€§èƒ½å·®å¼‚ | æå‡æ¯”ä¾‹ |
|---------|------------|----------|----------|---------|
| MXF4Ã—MXF4â†’F32 | 859.6 TFLOPS | 1,462.4 TFLOPS | +602.8 TFLOPS | **+70.1%** |
| NVF4Ã—NVF4â†’F32 | 859.4 TFLOPS | 1,463.6 TFLOPS | +604.2 TFLOPS | **+70.3%** |

**4ä½ç²¾åº¦å…³é”®å‘ç°**ï¼š
- RTX 5090 åœ¨4ä½ç²¾åº¦æ“ä½œä¸­è¡¨ç°å‡º**çº¦70%çš„æ€§èƒ½ä¼˜åŠ¿**
- ä¸¤æ¬¾æ˜¾å¡éƒ½èƒ½è¾¾åˆ°æé«˜çš„è®¡ç®—å¯†åº¦ï¼Œä½†RTX 5090æ˜æ˜¾æ›´ä¼˜
- 4ä½ç²¾åº¦æ˜¯AIæ¨ç†å’Œé‡åŒ–æ¨¡å‹çš„å…³é”®ç²¾åº¦æ ¼å¼

### 8ä½ç²¾åº¦æ“ä½œæ€§èƒ½å¯¹æ¯”

åœ¨8ä½ç²¾åº¦çŸ©é˜µä¹˜æ³•æ“ä½œä¸­ï¼Œæ€§èƒ½å·®å¼‚åŒæ ·æ˜æ˜¾ï¼š

| æ“ä½œç±»å‹ | RTX 5090 D | RTX 5090 | æ€§èƒ½å·®å¼‚ | æå‡æ¯”ä¾‹ |
|---------|------------|----------|----------|---------|
| MXF8Ã—MXF8â†’F32 | 426.5 TFLOPS | 731.5 TFLOPS | +305.0 TFLOPS | **+71.5%** |
| F8Ã—F8â†’F16 | 427.6 TFLOPS | 732.3 TFLOPS | +304.7 TFLOPS | **+71.3%** |
| INT8Ã—INT8â†’INT32 | 428.6 TFLOPS | 740.9 TFLOPS | +312.3 TFLOPS | **+72.9%** |

**8ä½ç²¾åº¦å…³é”®å‘ç°**ï¼š
- RTX 5090 åœ¨8ä½ç²¾åº¦æ“ä½œä¸­è¡¨ç°å‡º**çº¦71-73%çš„æ€§èƒ½ä¼˜åŠ¿**
- æ€§èƒ½æå‡å¹…åº¦ä¸4ä½ç²¾åº¦åŸºæœ¬ä¸€è‡´ï¼Œè¯´æ˜ç¡¬ä»¶æ¶æ„ä¼˜åŒ–æ˜¯å…¨é¢æ€§çš„
- 8ä½ç²¾åº¦åœ¨è®­ç»ƒå’Œæ¨ç†ä¸­éƒ½æœ‰é‡è¦åº”ç”¨

### æŠ€æœ¯åˆ†ææ€»ç»“

**ç¡¬ä»¶æ¶æ„å·®å¼‚**ï¼š
1. **è®¡ç®—å•å…ƒä¼˜åŒ–**ï¼šRTX 5090 ç›¸æ¯” RTX 5090 D åœ¨ä½ç²¾åº¦è®¡ç®—å•å…ƒä¸Šæœ‰æ˜¾è‘—ä¼˜åŒ–
2. **å†…å­˜å¸¦å®½**ï¼šè™½ç„¶ä¸¤è€…å…¨å±€å†…å­˜ç›¸åŒ(31.8 GiB)ï¼Œä½†RTX 5090çš„å†…å­˜å­ç³»ç»Ÿæ€§èƒ½æ›´ä¼˜
3. **å¤šå¤„ç†å™¨æ•ˆç‡**ï¼šç›¸åŒçš„170ä¸ªå¤šå¤„ç†å™¨ï¼Œä½†RTX 5090çš„è°ƒåº¦å’Œæ‰§è¡Œæ•ˆç‡æ›´é«˜

**åº”ç”¨åœºæ™¯å½±å“**ï¼š
1. **AIè®­ç»ƒ**ï¼šåœ¨æ··åˆç²¾åº¦è®­ç»ƒä¸­ï¼ŒRTX 5090èƒ½æä¾›70%ä»¥ä¸Šçš„é€Ÿåº¦ä¼˜åŠ¿
2. **æ¨ç†åŠ é€Ÿ**ï¼šé‡åŒ–æ¨¡å‹éƒ¨ç½²æ—¶ï¼ŒRTX 5090çš„ä¼˜åŠ¿æ›´åŠ æ˜æ˜¾
3. **æˆæœ¬æ•ˆç›Š**ï¼šRTX 5090åœ¨å•ä½æ—¶é—´å†…èƒ½å¤„ç†æ›´å¤šæ•°æ®ï¼Œæé«˜æ•´ä½“ååé‡

**ä¼˜åŒ–å»ºè®®**ï¼š
- å¯¹äºå¤§è§„æ¨¡AIå·¥ä½œè´Ÿè½½ï¼Œä¼˜å…ˆé€‰æ‹©RTX 5090
- RTX 5090 Dé€‚åˆé¢„ç®—æœ‰é™ä½†ä»éœ€è¦é«˜æ€§èƒ½è®¡ç®—çš„åœºæ™¯
- åœ¨4ä½å’Œ8ä½ç²¾åº¦ä¸ºä¸»çš„åº”ç”¨ä¸­ï¼Œä¸¤è€…æ€§èƒ½å·®å¼‚æœ€ä¸ºæ˜¾è‘—

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Request æ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ã€‚

## è®¸å¯è¯

MIT License



MMAPEAK is a CUDA-based benchmarking tool designed to measure the peak performance of matrix multiplication operations across various data types and tensor core configurations on NVIDIA GPUs.

## Overview

This tool measures the throughput of NVIDIA's Tensor Core dense operations using different precision formats:
- 4-bit integer (Int4)
- 4-bit floating point (FP4)
- 4-bit floating point with group scale (MXFP4 G32, NVFP4 G16)
- 6-bit floating point (FP6)
- 6-bit floating point with group scale (MXFP6 G32)
- 8-bit integer (INT8)
- 8-bit floating point (FP8)
- 8-bit floating point with group scale (MXFP8 G32)
- 16-bit floating point (FP16, BF16)
- 32-bit floating point (TF32)

## Building

### Using CMake

```bash
mkdir build && cd build
cmake ..
make
```a 

#### Note

Please use CUDA Toolkit version 12.8.1 (or later) instead of 12.8.0 to ensure compatibility with the Blackwell architecture.

`wgmma` is not currently utilized, results in suboptimal FP8 performance on Hopper devices.

## Usage

```bash
./mmapeak [options]
```

### Options

- `-t <seconds>`: Set target time for benchmarks in seconds (default: 3.0)
- `-h, --help`: Show help message

## Example Output

```
----------------------------------------
Device 0: NVIDIA H100 NVL
  Compute capability: 9.0
  Total global memory: 93.1 GiB
  Multiprocessor count: 132
Running benchmarks with target time: 3.0 seconds
mma_s4s4s32_8_8_32
run: 2998.6 ms 28.1 T(fl)ops
mma_mxf4mxf4f32_16_8_64
not supported
mma_nvf4nvf4f32_16_8_64
not supported
mma_f4f4f16_16_8_32
not supported
mma_f4f4f32_16_8_32
not supported
mma_f6f6f16_16_8_32
not supported
mma_f6f6f32_16_8_32
not supported
mma_mxf6mxf6f32_16_8_32
not supported
mma_mxf8mxf8f32_16_8_32
not supported
mma_f8f8f16_16_8_32
run: 3000.3 ms 1431.8 T(fl)ops
mma_f8f8f32_16_8_32
run: 2999.1 ms 1208.5 T(fl)ops
mma_s8s8s32_16_16_16
run: 2998.4 ms 1410.1 T(fl)ops
mma_s8s8s32_32_8_16
run: 2998.0 ms 1409.6 T(fl)ops
mma_f16f16f16_16_16_16
run: 2999.3 ms 992.3 T(fl)ops
mma_f16f16f16_32_8_16
run: 2999.5 ms 981.2 T(fl)ops
mma_f16f16f32_16_16_16
run: 2998.3 ms 978.4 T(fl)ops
mma_f16f16f32_32_8_16
run: 3001.9 ms 976.8 T(fl)ops
mma_bf16bf16f32_16_16_16
run: 2997.8 ms 972.9 T(fl)ops
mma_bf16bf16f32_32_8_16
run: 3000.0 ms 977.0 T(fl)ops
mma_tf32tf32f32_16_16_8
run: 2998.6 ms 380.6 T(fl)ops
```

## Compatibility

Tensor core operations that are not supported on your hardware will display "not supported".

## Architecture Support

- Turing (2080ti, etc.): SM75
- Ampere (A100, A30, etc.): SM80
- Ampere (A40, 3090, etc.): SM86
- Ada Lovelace (L40, 4090, etc.): SM89
- Hopper (H100, H200): SM90
- Blackwell (5090, etc.): SM120a

## License

This project is provided as-is.
